#!/bin/bash

#SBATCH --partition=@@queue@@
@@constraint@@
#SBATCH --account=mp107
#SBATCH --nodes=@@nodes@@
#SBATCH --time=@@time@@
#SBATCH --job-name=@@size@@-ground
#SBATCH --output=out_@@size@@_ground_vtune_@@machine@@_%j.log
#SBATCH --perf=vtune/2018.0

set -o errexit

echo Starting slurm script at $(date)

ulimit -s unlimited

export PYTHONNOUSERSITE=1
export HOME=${SCRATCH}

pstr=@@machine@@
outdir="out_@@size@@_ground_vtune_${pstr}"
mkdir -p "${outdir}"

# This script assumes that you are running at NERSC and have already
# loaded the toast module for the correct machine / configuration.

# This should be the same as the --nodes option above
nodes=@@nodes@@

# How many processes are we running per node?  Handle
# the case of a very small simulation.
if [ "x@@tiny@@" = "xyes" ]; then
    node_proc=1
else
    node_proc=@@nodeprocs@@
fi

# Generate the focalplane file if it does not already exist.

detpix=@@detpix@@

fpfile="${outdir}/fp_${detpix}.pkl"
fpparfile="@@topdir@@/params/ground/focalplane.par"

if [ ! -e "${fpfile}" ]; then
    srun -n 1 -N 1 toast_fake_focalplane.py @$fpparfile \
         --out "${outdir}/fp" --minpix $detpix
fi

# Generate the schedule file if it does not already exist.

schparfile="@@topdir@@/params/ground/schedule.par"

schedulefile="${outdir}/schedule.txt"

if [ ! -e "${schedulefile}" ]; then
    srun -n 1 -N 1 toast_ground_schedule.py @$schparfile \
         --out "${schedulefile}" --patch @@patch@@ \
         --stop @@schedule_stop@@
fi

# The executable script

ex=$(which toast_ground_sim.py)
echo "Using ${ex}"

# Scan strategy parameters from a file

parfile="@@topdir@@/params/ground/ground_sim.par"

# Data distribution parameters

groupsize=$(( @@nodes_per_group@@ * node_proc ))

# The commandline

com="${ex} @${parfile} \
--groupsize ${groupsize} \
--fp ${fpfile} \
--schedule ${schedulefile} \
--out ${outdir}/out \
--atm_cache ${outdir}/atm_cache \
--atm_xstep @@cellsize@@ \
--atm_ystep @@cellsize@@ \
--atm_zstep @@cellsize@@ \
"

#--- Hardware configuration (no need to change) ----

# Hyperthread CPUs per physical core
cpu_per_core=@@hyperthread@@

# Physical cores we are using
node_cores=@@nodecores@@

node_thread=$(( node_cores / node_proc ))
node_depth=$(( cpu_per_core * node_thread ))
procs=$(( nodes * node_proc ))

export OMP_NUM_THREADS=${node_thread}
export OMP_PLACES=threads
export OMP_PROC_BIND=spread

# Set TMPDIR to be on the ramdisk
export TMPDIR=/dev/shm

run="srun --cpu_bind=cores -n ${procs} -N ${nodes} -c ${node_depth}"

# only set "vrun" if not defined in environment (see vtune_params/ directory)
: ${vrun:="amplxe-cl -collect hotspots -knob sampling-interval=500 \
    -run-pass-thru=--no-altstack -r ${outdir}/vtune \
    -finalization-mode=deferred -trace-mpi"}
export vrun

echo Calling srun at $(date)

echo "${run} ${vrun} -r ${outdir}/vtune -- ${com}"
eval ${run} ${vrun} -r ${outdir}/vtune -- ${com} > "${outdir}/log" 2>&1

if [ ! -z "$(echo ${vrun} | grep -vi 'finalization-mode=full')" ]; then
    if eval command -v vfinal &> /dev/null ; then
        echo "Run \"vfinal <vtune-output-dir>\" to finalize results"
    else
        echo "No alias \"vfinal\" defined. Remember to finalize results"
    fi
fi

echo End slurm script at $(date)

